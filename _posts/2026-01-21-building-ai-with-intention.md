---
layout: post
title: "Building AI With Intention"
date: 2026-01-21
---

We live in a tech industry echo chamber.

Many of us, myself included, primarily interact with other tech enthusiasts. We speak the same language, share the same excitement, and move at the same, quick pace. We talk about models, agents, and efficiency gains as if they’re obvious and universal. Inside that world, it’s easy to only focus on what's next.

But outside our sphere, many people are experiencing this moment very differently.

My best friend doesn’t know what an agent is, and frankly, she doesn’t care. She doesn’t care about increasing efficiency or how many man-hours we can save per week. She doesn't care about vague promises of a personal assistant that will make her life easier, someday. She just wants to know why, today, it’s okay for companies to steal her artwork. To her, AI is just something that generates fake images and videos, often with deception in mind. How is it really helping _her_ specifically, right now?

This gap in thinking isn’t just anecdotal. Go to the comment section of any AI-generated content and you'll see the phrase "AI slop" repeated over and over. According to a recent survey, experts are way more optimistic about AI than the general public. While a majority of the experts are excited, only a small fraction of the public feels the same. Overall, concern outweighs excitement.

We often frame fear around AI in terms of jobs; automation and job replacement. And sure, that fear exists. But there are other fears too. Artists and writers worry about ownership and creativity. Privacy advocates worry about mass surveillance. Environmentalists worry about energy use and climate change. Most everyone worries about trust, authenticity, and whether they get a say in the systems shaping their lives.

This moment is often compared to the advent of the internet. It brought incredible progress, but also consequences - surveillance, manipulation, and abuse at scale. Now we’re here again, at another inflection point. I'm not going to pretend we can fix all the issues, but we shouldn't ignore them. This time, we have a chance to do even better.

That’s why intentionality matters. We need to build with intention.

You might say, “_But Mara, we already practice Responsible AI._” And that is great. But I’m talking about tangible impact and public perception. Right now, that trust is eroding. According to a Stanford survey last year, confidence that AI companies protect personal data is actually going down, and fewer people believe AI systems are unbiased and free from discrimination. There’s a gap between what we _think_ we’re doing and how it’s being perceived.

This isn’t about slowing progress or denying AI’s potential. It absolutely has massive potential. It is only a reminder that staying inside our own heads while we charge forward would be both naïve and irresponsible. It's easy to think these risks only apply to someone else, that what you're working on is different. Most of the time, harm doesn’t come from bad intentions - it comes from convenience, shortcuts, and moving fast without pausing.

You and I have real influence here, in what we build and in how we talk about it. Simple little decisions we make every single day. Do we treat concerns as signals worth listening to, or just obstacles in our way? Sometimes, responsibility looks surprisingly simple. It’s okay to say, “_Maybe AI isn’t the right solution for this._” Not every problem needs these tools, and using them just because we can isn’t the same thing as building with intention.

So if you’re going to use your newfound AI skills outside of your day job (and you should!) do something good with them. Don't just build solutions that increase profits for some company, create something that sparks real joy in someone. Build things that chip away at fear instead of reinforcing it. When people start to see real value done with real care, only then will the consequences start to feel worth it.
